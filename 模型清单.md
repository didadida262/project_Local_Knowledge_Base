# 项目模型清单

本文档列出了本地向量知识库项目中使用的所有AI模型和工具。

## 📊 模型总览

项目共使用 **2个主要AI模型**：

1. **嵌入模型（Embedding Model）** - 用于文本向量化
2. **推理模型（Inference Model）** - 用于AI问答生成

---

## 1. 嵌入模型（Embedding Model）

### 模型信息
- **模型名称**: `all-MiniLM-L6-v2`
- **模型类型**: Sentence Transformers
- **来源**: Hugging Face
- **向量维度**: 384维
- **文件位置**: `backend/vector_knowledge_base.py` (第22行)

### 使用方式
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = model.encode(text_chunks)
```

### 主要功能
- 将文档文本转换为向量表示
- 支持语义相似度计算
- 用于FAISS向量索引构建
- 支持多语言（包括中文）

### 代码位置
- **初始化**: `backend/vector_knowledge_base.py:42`
- **文档向量化**: `backend/vector_knowledge_base.py:89, 135`
- **查询向量化**: `backend/vector_knowledge_base.py:180`

### 依赖库
- `sentence-transformers>=2.5.0` (见 `backend/requirements.txt:3`)

---

## 2. 推理模型（Inference Model）

### 模型信息
- **模型名称**: `gemma2:2b`
- **模型类型**: Ollama本地大语言模型
- **服务地址**: `http://localhost:11434`
- **文件位置**: `backend/knowledge_retriever.py` (第18行)

### 使用方式
```python
# 通过Ollama API调用
response = requests.post(
    f"{ollama_url}/api/generate",
    json={
        "model": "gemma2:2b",
        "prompt": prompt,
        "stream": False
    }
)
```

### 主要功能
- 基于检索到的文档内容生成AI回答
- 支持中文问答
- 本地部署，保护隐私

### 代码位置
- **初始化**: `backend/knowledge_retriever.py:18`
- **调用生成**: `backend/knowledge_retriever.py:94-188`
- **模型检查**: `backend/api_server.py:484-516`

### 安装要求
```bash
# 需要先安装Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# 拉取模型
ollama pull gemma2:2b
```

### 依赖库
- `requests==2.31.0` (见 `backend/requirements.txt:17`)

---

## 3. 辅助工具（非模型）

### FAISS (向量索引库)
- **用途**: 高效的向量相似度搜索
- **类型**: 向量索引工具库
- **代码位置**: `backend/vector_knowledge_base.py:15, 67`
- **依赖**: `faiss-cpu>=1.9.0` (见 `backend/requirements.txt:4`)

### jieba (中文分词)
- **用途**: 中文文本分词处理
- **类型**: 中文分词工具库
- **代码位置**: `backend/document_processor.py:10`
- **依赖**: `jieba==0.42.1` (见 `backend/requirements.txt:14`)

---

## 🔄 模型工作流程

```
文档输入
    ↓
文档处理器 (jieba分词)
    ↓
嵌入模型 (all-MiniLM-L6-v2) → 生成384维向量
    ↓
FAISS向量索引 → 存储和检索
    ↓
用户查询 → 嵌入模型向量化
    ↓
FAISS相似度搜索 → 检索相关文档
    ↓
推理模型 (gemma2:2b) → 生成AI回答
    ↓
返回结果给用户
```

---

## 📦 模型依赖总结

### Python依赖 (`backend/requirements.txt`)
```txt
sentence-transformers>=2.5.0  # 嵌入模型
faiss-cpu>=1.9.0              # 向量索引
huggingface_hub>=0.16.0       # 模型下载
requests==2.31.0              # Ollama API调用
jieba==0.42.1                 # 中文分词
```

### 外部服务
- **Ollama**: 本地大语言模型服务
  - 默认地址: `http://localhost:11434`
  - 需要单独安装和启动

---

## 🔍 模型配置位置

### 嵌入模型配置
- **默认模型**: `backend/vector_knowledge_base.py:22`
- **可修改**: 在 `VectorKnowledgeBase.__init__()` 中修改 `model_name` 参数

### 推理模型配置
- **默认模型**: `backend/knowledge_retriever.py:18`
- **默认地址**: `backend/knowledge_retriever.py:18`
- **可修改**: 在 `KnowledgeRetriever.__init__()` 中修改 `ollama_model` 和 `ollama_url` 参数

---

## 📝 注意事项

1. **首次使用**: 
   - 嵌入模型会自动从Hugging Face下载（约80MB）
   - 推理模型需要手动通过Ollama安装

2. **模型存储**:
   - 嵌入模型: `~/.cache/huggingface/`
   - 推理模型: `~/.ollama/models/`

3. **网络要求**:
   - 首次下载嵌入模型需要网络连接
   - 推理模型通过Ollama本地运行，无需网络

4. **性能考虑**:
   - `all-MiniLM-L6-v2` 是轻量级模型，适合本地部署
   - `gemma2:2b` 是2B参数的小模型，适合资源受限环境

---

## 🎯 模型替换建议

### 替换嵌入模型
如果需要更好的中文支持，可以考虑：
- `paraphrase-multilingual-MiniLM-L12-v2` (多语言，768维)
- `text2vec-chinese` (中文优化)

### 替换推理模型
如果需要更强的推理能力，可以考虑：
- `gemma2:9b` (更大模型，需要更多资源)
- `llama3:8b` (Meta的Llama模型)
- `qwen2:7b` (阿里通义千问，中文优化)

---

**生成时间**: 2025-01-27
**项目版本**: v1.0.0

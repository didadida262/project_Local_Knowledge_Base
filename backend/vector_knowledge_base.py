#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘é‡çŸ¥è¯†åº“
ä½¿ç”¨Sentence Transformerså’ŒFAISSå®ç°å‘é‡åŒ–å­˜å‚¨å’Œæ£€ç´¢
"""

import os
import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple
from sentence_transformers import SentenceTransformer
import faiss
from document_processor import DocumentProcessor


class VectorKnowledgeBase:
    """å‘é‡çŸ¥è¯†åº“ç±»"""
    
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", storage_dir: str = "./knowledge_base"):
        """
        åˆå§‹åŒ–å‘é‡çŸ¥è¯†åº“
        
        Args:
            model_name: å¥å­è½¬æ¢æ¨¡å‹åç§°
            storage_dir: å­˜å‚¨ç›®å½•
            use_reranker: æ˜¯å¦ä½¿ç”¨é‡æ’æ¨¡å‹
        """
        self.model_name = model_name
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(exist_ok=True)
        # åˆå§‹åŒ–æ¨¡å‹
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {model_name}")
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡å¢åŠ è¶…æ—¶æ—¶é—´ï¼ˆåœ¨å¯¼å…¥SentenceTransformerä¹‹å‰è®¾ç½®ï¼‰
            os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '300'  # 5åˆ†é’Ÿè¶…æ—¶
            
            # å°è¯•åŠ è½½æ¨¡å‹
            # SentenceTransformerä¼šè‡ªåŠ¨ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œå¦‚æœæ¨¡å‹å·²ä¸‹è½½åˆ™ä¸ä¼šé‡æ–°ä¸‹è½½
            self.model = SentenceTransformer(model_name)
            self.dimension = self.model.get_sentence_embedding_dimension()
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            error_msg = str(e)
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {error_msg}")
            print("=" * 60)
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
            if "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                print("   ç½‘ç»œè¿æ¥è¶…æ—¶ï¼Œè¯·å°è¯•:")
                print("   1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
                print("   2. å¦‚æœæ¨¡å‹å·²ä¸‹è½½ï¼Œæ£€æŸ¥ç¼“å­˜ç›®å½•: ~/.cache/huggingface/")
                print("   3. å¯ä»¥æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç¼“å­˜")
            elif "connection" in error_msg.lower():
                print("   ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥:")
                print("   1. æ˜¯å¦å¯ä»¥è®¿é—® huggingface.co")
                print("   2. æ˜¯å¦éœ€è¦é…ç½®ä»£ç†")
            else:
                print("   è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶å°è¯•:")
                print("   1. é‡æ–°å¯åŠ¨æœåŠ¡")
                print("   2. æ£€æŸ¥æ¨¡å‹åç§°æ˜¯å¦æ­£ç¡®")
            print("=" * 60)
            raise
        
        # åˆå§‹åŒ–FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(self.dimension)  # å†…ç§¯ç›¸ä¼¼åº¦
        self.documents = []
        self.chunks = []
        
        # åŠ è½½å·²å­˜åœ¨çš„çŸ¥è¯†åº“
        self._load_knowledge_base()
    
    def add_document(self, file_path: str) -> Dict[str, Any]:
        """
        æ·»åŠ å•ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            processor = DocumentProcessor()
            doc_info = processor.process_document(file_path)
            
            # ç”Ÿæˆå‘é‡
            embeddings = self.model.encode(doc_info['chunks'])
            
            # æ·»åŠ åˆ°FAISSç´¢å¼•
            self.index.add(embeddings.astype('float32'))
            
            # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
            doc_id = len(self.documents)
            doc_info['doc_id'] = doc_id
            doc_info['chunk_start'] = len(self.chunks)
            doc_info['chunk_end'] = len(self.chunks) + len(doc_info['chunks'])
            
            self.documents.append(doc_info)
            
            # ä¿å­˜æ–‡æœ¬å—
            for i, chunk in enumerate(doc_info['chunks']):
                self.chunks.append({
                    'doc_id': doc_id,
                    'chunk_id': i,
                    'text': chunk,
                    'embedding': embeddings[i].tolist()
                })
            
            print(f"âœ… æ–‡æ¡£å·²æ·»åŠ : {doc_info['file_name']} ({doc_info['chunk_count']} å—)")
            return doc_info
            
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {file_path} - {str(e)}")
            raise
    
    def add_directory(self, directory_path: str) -> List[Dict[str, Any]]:
        """
        æ·»åŠ ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æ¡£
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        processor = DocumentProcessor()
        documents = processor.process_directory(directory_path)
        
        results = []
        for doc_info in documents:
            try:
                # ç”Ÿæˆå‘é‡
                embeddings = self.model.encode(doc_info['chunks'])
                
                # æ·»åŠ åˆ°FAISSç´¢å¼•
                self.index.add(embeddings.astype('float32'))
                
                # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
                doc_id = len(self.documents)
                doc_info['doc_id'] = doc_id
                doc_info['chunk_start'] = len(self.chunks)
                doc_info['chunk_end'] = len(self.chunks) + len(doc_info['chunks'])
                
                self.documents.append(doc_info)
                
                # ä¿å­˜æ–‡æœ¬å—
                for i, chunk in enumerate(doc_info['chunks']):
                    self.chunks.append({
                        'doc_id': doc_id,
                        'chunk_id': i,
                        'text': chunk,
                        'embedding': embeddings[i].tolist()
                    })
                
                results.append(doc_info)
                print(f"âœ… æ–‡æ¡£å·²æ·»åŠ : {doc_info['file_name']} ({doc_info['chunk_count']} å—)")
                
            except Exception as e:
                print(f"âŒ æ·»åŠ æ–‡æ¡£å¤±è´¥: {doc_info['file_name']} - {str(e)}")
        
        return results
    
    def search(self, query: str, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        æœç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°é‡
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if len(self.chunks) == 0:
            return []
        
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = self.model.encode([query])
        
        # æœç´¢ç›¸ä¼¼å‘é‡
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # æ„å»ºç»“æœ
        results = []
        for score, idx in zip(scores[0], indices[0]):
            # ç¡®ä¿ç´¢å¼•æ˜¯Python intç±»å‹
            idx = int(idx)
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                doc = self.documents[chunk['doc_id']]
                
                results.append({
                    'chunk_id': int(idx),
                    'doc_id': int(chunk['doc_id']),
                    'file_path': str(doc['file_path']),
                    'file_name': str(doc['file_name']),
                    'text': str(chunk['text']),
                    'similarity': float(score),
                    'chunk_index': int(chunk['chunk_id'])
                })
        
        return results
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        total_chunks = len(self.chunks)
        total_documents = len(self.documents)
        unique_files = len(set(doc['file_path'] for doc in self.documents))
        
        return {
            'total_vectors': int(total_chunks),
            'total_documents': int(total_documents),
            'unique_files': int(unique_files),
            'model_name': str(self.model_name),
            'dimension': int(self.dimension)
        }
    
    def get_documents(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯"""
        return [
            {
                'file_path': str(doc['file_path']),
                'file_name': str(doc['file_name']),
                'chunk_count': int(doc['chunk_count']),
                'word_count': int(doc['word_count']),
                'file_size': int(doc['file_size'])
            }
            for doc in self.documents
        ]
    
    def save_knowledge_base(self):
        """ä¿å­˜çŸ¥è¯†åº“åˆ°ç£ç›˜"""
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, str(self.storage_dir / "faiss_index.bin"))
        
        # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
        with open(self.storage_dir / "documents.json", 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ–‡æœ¬å—
        with open(self.storage_dir / "chunks.json", 'w', encoding='utf-8') as f:
            json.dump(self.chunks, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜é…ç½®
        config = {
            'model_name': self.model_name,
            'dimension': self.dimension,
            'total_documents': len(self.documents),
            'total_chunks': len(self.chunks)
        }
        with open(self.storage_dir / "config.json", 'w', encoding='utf-8') as f:
            json.dump(config, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çŸ¥è¯†åº“å·²ä¿å­˜åˆ°: {self.storage_dir}")
    
    def _load_knowledge_base(self):
        """ä»ç£ç›˜åŠ è½½çŸ¥è¯†åº“"""
        config_file = self.storage_dir / "config.json"
        if not config_file.exists():
            # çŸ¥è¯†åº“ä¸ºç©ºï¼Œä¸è¾“å‡ºæç¤ºä¿¡æ¯
            return
        
        try:
            # åŠ è½½é…ç½®
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            # åŠ è½½FAISSç´¢å¼•
            index_file = self.storage_dir / "faiss_index.bin"
            if index_file.exists():
                self.index = faiss.read_index(str(index_file))
            
            # åŠ è½½æ–‡æ¡£ä¿¡æ¯
            docs_file = self.storage_dir / "documents.json"
            if docs_file.exists():
                with open(docs_file, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
            
            # åŠ è½½æ–‡æœ¬å—
            chunks_file = self.storage_dir / "chunks.json"
            if chunks_file.exists():
                with open(chunks_file, 'r', encoding='utf-8') as f:
                    self.chunks = json.load(f)
            
            # çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œç»Ÿè®¡ä¿¡æ¯ä¼šåœ¨api_serverä¸­æ˜¾ç¤º
            pass
            
        except Exception as e:
            print(f"âš ï¸ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {str(e)}")
            print("ğŸ“ å°†åˆ›å»ºæ–°çš„çŸ¥è¯†åº“")
    
    def clear_knowledge_base(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        self.index = faiss.IndexFlatIP(self.dimension)
        self.documents = []
        self.chunks = []
        
        # åˆ é™¤å­˜å‚¨æ–‡ä»¶
        for file in self.storage_dir.glob("*"):
            file.unlink()
        
        print("ğŸ—‘ï¸ çŸ¥è¯†åº“å·²æ¸…ç©º")
